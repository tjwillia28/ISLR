{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Chapter 2: Statistical Learning\n",
    "\n",
    "- **Input variables** are denoted  $X_{i}$ where *i* is used to distinguish the different inputs\n",
    "- Inputs can also go by *predictors, independent variables, features, or just variables*\n",
    "\n",
    "- **Output variables** are denoted by $Y$\n",
    "- Outputs can also go by *repsonse or dependent variable*\n",
    "\n",
    "- Let $X_{0}, X_{1}, ... , X_{P}$ be input variables. If $X = (X_{0}, X_{1}, ... , X_{P})$ , then we can assume there is some relationship between $Y$ and $X$ s.t. $Y = f(X) + \\epsilon$ where $f$ is some fixed, unknown function and $\\epsilon$ is a random error term with mean 0 and that is independent of $X$\n",
    "- Statistical learning refers to the set of approaches to estimate $f$\n",
    "- We are interested in estimating $f$ for **prediction** and **inference**\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "source": [
    "   - *Prediction*:\n",
    "       \n",
    "       - Let $\\hat{Y}$ and $\\hat{f}$ represent our estimate for the output and function, respectively.\n",
    "        \n",
    "        - $\\hat{f}$ is often treated as a *black box* meaning that practicioners are **not typically concerned with the exact form of $\\hat{f}$ so long as it yields accurate predictions for Y**\n",
    "        \n",
    "        - **NOTE**: the author references ignoring the error term because they average out to 0, which is fine. Just got me thinking about error assumptions... In order to gain that insight analyzing error terms don't you first have to run the model including the error? You aren't aware initially that the error distribution would have such favorable properties. How do you perform any worthwhile error analysis with a clumsy model? or is it just part of the iterative cycle?\n",
    "       \n",
    "       - The accuracy of $\\hat{Y}$ as a prediction for $Y$ depends two quantities, **reducible error** and **irreducible error**  \n",
    "          \n",
    "          - **Reducible Error**: our goal is to fit $\\hat{f}$ as well to $f$ using the appropriate statistical learning techniques.  Even then, our model will still have some error in it.\n",
    "           \n",
    "           - **Irreducible Error**: We are trying to fit our model on the equation $\\hat{Y} = f(X)$, which doesn't explicity include $\\epsilon$\n",
    "        - Consider $\\hat{Y} = \\hat{f}(X)$ and let $\\hat{f}$ and $X$ be fixed, then: <br \\>\n",
    "\n",
    " $$\\mathbb{E}(Y - \\hat{Y})^{2} = \\mathbb{E}[f(X)  - \\epsilon + \\hat{X}]^{2} = [f(X) - \\hat{f}(X)]^{2} + Var(\\epsilon)$$\n",
    "\n",
    "      - Lets go into the math above since this shit matters.\n",
    "      - $[f(X) - \\hat{f}(X)]^{2}$: \n",
    "          - is the term that measures predictive accuracy of our model.  How exactly it does so will depend on the type of task, i.e. classification, regression\n",
    "          - Reducible error, which is good... so you're saying there's a chance!\n",
    "          - Our goal is to control what we can, and reduce this.  Goal of Supervised Learning\n",
    "   \n",
    "     - $Var(\\epsilon)$:\n",
    "         - variance of the error\n",
    "         - this is irreducible error\n",
    "         - **irreducible error will always provide an upped bound on the accuracy of our prediction for $Y$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
